---
title: "Boots_OriginalCode_brennas"
author: "brennastallings"
date: "11/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This is for [Homework 5](https://fuzzyatelin.github.io/bioanth-stats/homework-05.html)

Comentary: You can use the packages simple boot and boot to run your boostrap and calculate CIs. These packages will make life much easier. 

First we call any libraries we need
```{r}
library(ggplot2)
library(curl)
library(lmodel2)
library(dplyr)
library(boot)
library(simpleboot)
```
Then we call in the data set needed
```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d)
```
The we start the fun

>[1] 

#Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).
```{r}
m<- lm(log(HomeRange_km2)~log(Body_mass_female_mean), data=d)
m
```
β0=-9.441
β1=1.036

>[2] 

#Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.
```{r echo=F}
#this is no longer needed
#samp_b0<- NULL
#samp_b1<- NULL
#n<- 50
#for (i in 1:1000){
#  d[i]<- sample_n(d, n, replace=T)
#  m[i]<- lm(log(HomeRange_km2)~log(Body_mass_female_mean), data=d[i])
#  samp_b0[i]<-m[i]$coefficients[1]
#  samp_b1[i]<-m[i]$coefficients[2]
#}

```
This isnt working at the moment, I googled the error and it didnt help. It works if I do each by hand, but cant add to samp
```{r}
#this is me trying boot
# I need a function for the stats
boot.fxn<- function(data, indices){
    d<-data[indices, ]
    m1<- lm(data= d, log(HomeRange_km2)~log(Body_mass_female_mean))
    return(coef(m1))
    }
out<- boot(d, statistic = boot.fxn, R=1000)
```
#Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap
```{r}
out
```
t1=0.605
t2=0.077


```{r echo=F}
#I made a sample set to test math with
#samp.test<- rbind(samp1, samp2)

#sd(samp_b0)
#sd(samp_b1)

#this is for 'b0'
#sd(samp.test[,1])
#this is for 'b1'
#sd(samp.test[,2])
```


#Determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
boot.ci(out)
```
Normal CI is -10.626,  -8.251
```{r echo=F}
#test code
#t <- c(mean(samp.test[,1]), sd(samp.test[,1]), (mean(samp.test[,1])/sd(samp.test[,1])))
#t1<- c(mean(samp.test[,2]), sd(samp.test[,2]), (mean(samp.test[,2])/sd(samp.test[,2])))
#t<- rbind(t, t1)
#colnames(t) <- c("Est", "SE","t")
#t<- as.data.frame(t)
#t$lower <- t$Est - qt(0.975, df = 998) * t$SE
#t$upper <- t$Est + qt(0.975, df = 998) * t$SE
#ci <- c(t$lower, t$upper)  # by hand
#ci
```


#How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?
For the entire dataset:
```{r echo=F}
summary(m)
```
B0=0.672
B1=0.084
```{r echo=F}
x<- c("","Dataset", "Sample")
y<- c("B0", 0.672, 0.605)
z<- c("B1", 0.084, 0.077)
summ1<- rbind(x,y,z)
summ1
```
The numbers for the dataset as a whole are slightly larger than those for the sample

#How does the latter compare to the 95% CI estimated from your entire dataset?
Entire Dataset:
```{r}
ci <- confint(m, level = 0.95)  # using the results of lm()
ci
```

```{r echo=F}
x<- c("","Dataset", "Sample")
u<- c("B0 Upper", -8.110, -8.215)
l<- c("BO Lower", -10.772, -10.626)
v<- c("B1 Upper", 1.204,0 )
o<- c("B1 Lower", 0.869, 0)
summ2<- rbind(x,l,u,o,v)
summ2
```
The CIs from the entire dataset are larger than the ones for the sample
>5 Challenges

1.Obviously I have serious issues with my For Loop-Which has been removed
</br>
2. I figured out how to get the CI for B0, but not B1 from boot.ci
</br>
3. Im still somewhat confused by the boots package
</br>
4. I feel like the CIs for the data should have been smaller than for the sample. Is it becasue the sample has more points to use to calculate it?
</br>
5. Not sure there is a 5th one right now